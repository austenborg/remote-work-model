{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is Project Milestone 2!\n",
    "\n",
    "\n",
    "#### 1. Project Plan (28 points)\n",
    "Discuss as a team and make a plan for your final project! Share the following information in the writeup:\n",
    "1. The progress time schedule and milestones before project due date. (10 points)\n",
    "2. How is the work going to be shared among the team members? (8 points)\n",
    "3. Identified challenges and tentative solutions (10 points)\n",
    "4. Optional: preliminary results (extra credit: 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Time Schedule and Milestones\n",
    "\n",
    "Week 3/28-4/3\n",
    "* Review: Github\n",
    "* Write outline for report\n",
    "* Write Introduction section of report\n",
    "* Format Kaggle data, including identifying if a job is remote or not remote and the posting date. Change level of analysis to days, measuring the ratio of remote/non-remote jobs published on each date\n",
    "\n",
    "Week 4/4-4/10\n",
    "* Write Background section of report\n",
    "* Build initial model and assess performance\n",
    "* Test inclusion of different predictors, potentially including text indicators and location\n",
    "\n",
    "Week 4/11-4/17\n",
    "* Write Methodology section of report\n",
    "* Write Progress Report 1 (DUE April 16)\n",
    "* Debugging and refactoring\n",
    "\n",
    "Week 4/18-4/24\n",
    "* Write Results and Discussion section of report\n",
    "* Create presentation (DUE April 20-May 3)\n",
    "* Finalizing Code (DUE May 7)\n",
    "\n",
    "Week 4/25\n",
    "* Write Progress Report 2 (DUE April 30)\n",
    "* Write Conclusion section of report\n",
    "* Finalizing Code (DUE May 7)\n",
    "* Finalize Final Report (DUE May 7)\n",
    "\n",
    "(2) Division of Work\n",
    "\n",
    "* Pair programming for the coding section\n",
    "* Assign debugging and refactoring as needed\n",
    "* Austen: Background, Results and Discussion, Conclusion\n",
    "* Sophia: Methodology, Results and Discussion, Conclusion\n",
    "\n",
    "(3) Identified Challenges and Tentative Solutions\n",
    "\n",
    "* Problem 1: A big part of this dataset is the text in the job postings and company descriptions. With text data, there are a lot of things we could look for, but we can’t know ahead of time which will be useful predictors.\n",
    "* Solution: We will start with a few more straightforward text features, the presence of “remote”, and eventually we will test related words and phrases, like “hybrid” or “flexible work schedule”. If we have time, we can look at other elements of the text, like length, formality, and “soft skills”. \n",
    "\n",
    "* Problem 2: We may have trouble writing our regression code from scratch, especially the table formatting code part.\n",
    "* Solution: We can refer to previous homeworks as a model and template, and consult our professor as needed.\n",
    "\n",
    "* Problem 3: Currently, we are using one dataset containing data from 3 months. The narrow scope of the data might skew our results, and there might not be an even distribution within the time frame. \n",
    "* Solution: If variables are not distributed evenly the dates, we could use data augmentation to increase the number of training and testing observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Literature Review (72 points)\n",
    "\n",
    "Recall your group's list of selected papers from [the previous milestone](https://moodle.mtholyoke.edu/mod/glossary/view.php?id=694157). You should have picked out 2*n papers (where n is the number of group members) to read thoroughly. \n",
    "\n",
    "For each paper chosen, answer the following questions:\n",
    "1. What venue was the paper published in? Who are the authors? What are their backgrounds? How many times has the paper been cited? (~3-5 sentences)\n",
    "2. What problem are the authors trying to solve? (~2 sentences)\n",
    "3. Why is the problem important (2-5 sentences)?\n",
    "4. What mathematical notation do the authors use and what does this notation mean? This is often most useful represented as a table.\n",
    "5. Make a list of five terms that you do not understand from your paper. For each, do a bit of research and write a brief description of the term. (~2 sentences each.)\n",
    "6. What datasets did the authors use (if any)? Why did they use these datasets? (~2 sentences)\n",
    "\t1. What machine learning models did the authors use (if any)? Why did they justify using these? (~2 sentences)\n",
    "\t2. How did the authors know their approach was successful? (~10 sentences)\n",
    "7. What evaluation metrics did they use? What do these metrics mean?\n",
    "\t1. What baselines did they compare against?\n",
    "\t2. Can you identify a problem with the authors' measure of success?\n",
    "8.  Research papers are almost always an improvement, reaction or twist on other research (“prior work”) that others have done before. Of all of the works cited in your paper, what seems to be the most important cited prior work? Explain why that citation seems most important. (4-6 sentences)\n",
    "9.  Research papers almost always make use of tools, methods and algorithms that have been developed by others. Of all of the works cited in your paper, what citation of a tool, method or algorithm seems most important? Explain why. (4-6 sentences)\n",
    "10. Pick one equation from the paper and explain what it means using a mix of prose and mathematical notation (~5 sentences).\n",
    "11. How is your project similar and different from this paper? (4-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Paper: \"Is remote work in high demand? Evidence from job postings during COVID-19\"\n",
    "https://dl.acm.org/doi/abs/10.1145/3460112.3471984\n",
    "\n",
    "1. The paper was presented in June of 2021 at the COMPASS ‘21: ACM SIGCAS Conference on Computing and Sustainable Societies. The authors are Jose Morales-Arilla and Carlos Daboin. Jose Morales-Arilla is a PhD Candidate in Public Policy (Economics Track) at Harvard University, as well as a Doctoral Fellow at Harvard's Growth Lab and a Visiting Researcher at The Brookings Institution. Carlos Daboin is an Economics and Analytics consultant for the Workforce of the Future initiative at The Brookings Institution. The paper has not yet been cited.\n",
    "\n",
    "2. The authors were first trying to determine if remote work is in high demand based on evidence from job postings during COVID-19. Although all job postings decreased during the pandemic, the expectation is that job postings for non-remote work will have decreased less. However, while employment remained steady, job postings dropped. The authors then explore several possible explanations.\n",
    "\n",
    "3. It is useful to examine the diverging outcomes for workers performing “remotable” and “not-remotable” work, which is an important part of the overall economic response to the pandemic. Remote work is also a significant socio-economic indicator, as non-remote workers are less educated, have lower incomes, have fewer liquid assets, and are less likely to be home-owners. Classifying job postings by remote and not-remote can account for discrepancies in the resilience of certain sections of the job market. Knowing the job market for remote work can help job seekers access their own employment prospects. More broadly, the pandemic and job posting data during it are an important if not turning point in the larger evolution and history of employment and remote work.\n",
    "\n",
    "4. Mathematical notation:\n",
    "β (beta): not a function, represents a variable and coefficient.\n",
    "Σ (sigma): summation symbol; the sum of multiple terms.\n",
    "log W_r: the natural logarithm of the reported wage of respondent r\n",
    "Ex_r: the potential years of work experience\n",
    "Ed_r: the educational level\n",
    "G_r: the gender\n",
    "φ_o: a binary indicator for occupation o\n",
    "α_o: captures the marginal compensation to an additional year of potential experience for occupation o.\n",
    "αˆ_o: estimates as our measure of the return to experience in occupation o.\n",
    "NRsh_i: non-remotable workers in each industry\n",
    "shio: weighted average by occupation, where the share of non-remotable jobs subtracts each occupation’s share for a industry\n",
    "NR_o = 1: if that occupation is itself non-remotable\n",
    "Y_ot: marks the value of the index of employment or job postings for occupation o at month t.\n",
    "R_o: is a binary variable that marks whether occupation o is remotable\n",
    "Post_t: a binary variable that marks whether month t is after President Trump’s National Emergency Declaration of March 13, 2020. \n",
    "X_o: a matrix of K co-variate controls\n",
    "ε_ot: the error term\n",
    "α: main coefficient of interest. \n",
    "\n",
    "5. Terms and definitions:\n",
    "Orthogonal: In statistics, orthogonal means uncorrelated. Independent variables that do not affect each other.\n",
    "Confounder: A confounding variable influences both the dependent and the independent variables. This causes a false association that incorrectly implies causation.\n",
    "Metadata schema: Metadata is data about data. A schema is a common understanding or layout of this data.\n",
    "Crosswalk (data): A crosswalk is a document mapping the relationships between fields of different metadata schemas. This allows metadata to be shared between data types and structures.\n",
    "Baseline (research): A baseline is a study done at the beginning of the project to collect data or information about the situation or subject. This can be compared with data collected during and after the project.\n",
    "\n",
    "6. The main data sets used by the authors are the monthly estimates of employment by occupation from the IPUMS Current Population Survey (CPS) from January 2018 to December 2020, and the monthly series of job postings by occupation from Burning Glass Technologies (BGT), a labor market analytics company that collects online job vacancies posted across multiple job boards and company websites on a daily basis. They used CPS because, although it has a smaller sample size, it includes occupation-level employment estimates, as well as information about the reason for unemployment. This allows for deeper insight into recent labor market shifts, and allows the researchers to identify job separations, counted as layoffs.\n",
    "\n",
    "7. What evaluation metrics did they use? What do these metrics mean?\n",
    "\n",
    "1. There were no machine learning models used in this research. The authors’ results were found through data aggregation and statistical methods.\n",
    "\n",
    "2. The authors know their approach was successful because they measured demand for new hires using rigorous statistical methods, they controlled for confounding variables and explored differences across job posting attributes. They not only looked at trends in demand for remote versus not-remote workers, but considered several possible explanations for apparent contradictions. One potential explanation is that as businesses started to reopen, they attempted to hire back for the layoffs they had at the onset of lockdowns, but after controlling for layoffs, this is apparently not the case. The effects on remotable work is also not due to an industrial recomposition of the labor market during the pandemic, since results were similar across different sectors of the economy. Additionally, their approach produced results similar to or at least non-contradictory with other research papers. Some of the cited research has suggested remote job postings have not increased significantly or in fact decreased significantly after the initial surge of the pandemic. Beyond the statistical methods themselves, deeper inquiry into the causes that would justify these results, and the results of another cited paper, additional research would need to be done to verify the success of this research.\n",
    "\n",
    "1. The authors did not appear to measure their results against any baseline, other than results of prior studies.\n",
    "\n",
    "2. I did not identify any problems with the authors’ measure of success, other than lack of baselines.\n",
    "\n",
    "8.  The most important prior work cited in this paper was \"COVID-19 and remote work: an early look at US data.\" This was a 2020 technical report by Erik Brynjolfsson, John J Horton, Adam Ozimek, Daniel Rock, Garima Sharma, and Hong-Yi TuYe with the National Bureau of Economic Research. Of all the papers cited, it had the most similar research question and objectives to Morales-Arilla and Daboin’s paper. However the data in this prior work was collected via surveys of a representative sample of US workers, whereas the data in this paper was sourced from a combination of household survey and collection of job postings and vacancies. Self-reported data from workers and data from the job postings of companies will be influenced by different factors, so a combination should present a broader and clearer picture of the actual status of remote work in the job market.\n",
    "\n",
    "9.  This paper uses a “Mincer-like regression” algorithm, named after Jacob Mincer. This refers to the Mincer earnings function, a model that consists of a single equation, expressing wage income as a function of schooling and experience. In this paper, the variables used were the respondent’s employment status, occupation, education attainment, age, and total pre-tax wage and salary income. This algorithm was critical for estimating the returns to experience, a factor that could explain divergence in employment and remotable jobs.\n",
    "\n",
    "10. The first equation of the paper, under Returns to Experience:\n",
    "\n",
    "log W_r = β_1*(Ex_r) + Σ(φ_o) + Σ(α_o)*Ex_r * φ_o + β_2*(Ed_r) + β_3*(G_r) + ε_r\n",
    "\n",
    "This equation calculates the natural logarithm of the reported wage of respondent r (log W_r). It is essentially a sum that includes the potential years of work experience (Ex_r), the educational level (Ed_r), and the gender (G_r) of the respondent. It also accounts for the marginal compensation for each additional year of experience for an occupation (α_o). Lastly, the variable αˆ_o estimates as our measure of the return to experience in the occupation.\n",
    "\n",
    "11. Though we are researching the same topic, our project will differ from this paper in several ways. We will use a data set of job postings pulled from the job site Indeed, instead of data sets gathered by data and analytics organizations. Our project will also only process data on remote work job postings, not remote layoffs. Additionally, our research will be more concerned with predicting these trends than explaining them. We also won’t limit our scope to the height of the pandemic, but consider job posting data after 2020.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Paper: \"Investigating the Demand for Blockchain Talents in the Recruitment Market: Evidence from Topic Modeling Analysis on Job Postings\"\n",
    "https://www.sciencedirect.com/science/article/pii/S0378720621000872\n",
    "\n",
    "1. The paper has been accepted by and will be published in the Journal of Information & Management, Volume 59, Issue 4. The authors are Chunmian Ge, Professor in the Department of Financial Management, School of Business Administration, South China University of Technology (SCUT), who holds a Ph.D. in Information Systems; Haoyue Shi, a Ph.D. candidate of the School of Business Administration, SCUT; Junhui Jiang, an Associate Professor in the Department of Technological Economics & Management, School of Business Administration, SCUT, who holds a Ph.D. in Information Systems; and Xiaoying Xu, an Associate Professor in the Department of Decision Science, School of Business Administration, and director of the SCUT Fala Blockchain Research Center. The paper has not yet been cited.\n",
    "\n",
    "2. The authors seek to understand what qualifications and skills employers value and demand from blockchain workers in the job market in China. Their paper aims to reveal information about how highly these different skills are valued by evaluating salary levels, and about broader demand for blockchain talent by analyzing how the number of job postings change over time.\n",
    "\n",
    "3. This research is important for a number of reasons. On the job seeker side, it is useful research for IT professionals who need to keep their skills up to date, and who in particular face frequent changes in technological requirements and business practices. On the researcher side, this paper will enrich the overall literature on talent recruitment, the IT industry, and the labor market. Additionally, there has been little to no research into the actual skills and qualifications called for in blockchain technology. This paper is one of the first to do so.\n",
    "\n",
    "4. Mathematical notation:\n",
    "\n",
    "β (beta): not a function, represents a variable and coefficient.\n",
    "Σ (sigma): summation symbol; the sum of multiple terms.\n",
    "\n",
    "5. Terms and definitions:\n",
    "Blockchain: A blockchain is a digitally distributed, decentralized, public ledger that exists across a network, where the database is shared by the nodes of the network. It is most commonly and famously used for cryptocurrency systems.\n",
    "Topic Coherence: Topic coherence is a metric of the quality of a topic. The quality of a topic means how the more cohesive and interpretable the topic is, and is indicated by a high topic coherence.\n",
    "Perplexity: Perplexity measures the predictive power, or generalizability, of the topic modeling algorithm used for new documents. Higher perplexity indicates worse potential of the model to generalize for new documents; higher perplexity indicates a better predictive ability of the model. \n",
    "OLS: OLS just stands for Ordinary Least Squares, a regression model using the most common version of least squares. Other types are Weighted Least Squares (WLS) and Generalized Least Squares (GLS).\n",
    "T-shaped: T-shaped refers to professionals and workers who have significant experience and skills in one area and a breadth of other skills they don’t have as deep a knowledge of. The depth of knowledge in one area is represented by the vertical stroke of the “T” and the breadth by the horizontal stroke of the “T”.\n",
    "\n",
    "6. For their dataset, the researchers created their own job posting crawler to collect all job posting data from a nationwide Chinese recruitment website since February 2017. Searching for “blockchain” in job titles, job descriptions, or job requirements, their crawler collected 18,028 job postings for their data set. Collecting their own data has obvious benefits, and they used this particular recruitment website because, besides being very popular, it thoroughly vets every firm that registers as an employer to post jobs, and the website provides a structured template for these job postings, making it a relatively reliable and complete data source.\n",
    "\n",
    "1. The authors used the machine learning model of ordinary least squares (OLS) regression to analyze the coefficient between identified skill set and average salary. Since salary is a linear relationship with the number or level of skills or qualifications a worker has, a linear model is justified. They also use a topic modeling algorithm called Latent Dirichlet Allocation (LDA), which identifies potential topics in the job postings for the researchers automatically and without previously defined labels.\n",
    "\n",
    "2. The authors’ approach was successful because they used rigorous statistical and machine learning methods, and enlisted additional help as needed. For instance, to obtain more accurate and objective names and interpretations of these skill sets, the authors had students not involved with the study but familiar with IT research to independently label the skill sets based on the keywords that the LDA dealt with. Only after they had identified these skill sets using a combination of human and machine learning methods, did they set out to determine the relationships between skill set and demand, i.e. salary. And instead of only looking at blockchain related skills and salaries, the researchers took into account a series of other factors that affected salary to more accurately uncover the relationship. The machine learning techniques used were fully justified by the needs of the research. Regression was used for the linear relationships of salary and time, respectively. LDA was used for the classification of large sets of data to determine divisions in the type of blockchain technology work. The paper also achieved its primary goal; to provide more research and resources for insight into the market for blockchain technology. \n",
    "\n",
    "7. What evaluation metrics did they use? What do these metrics mean?\n",
    "\n",
    "1. The authors did not appear to measure their results against any baseline, other than results of prior studies.\n",
    "\n",
    "2. I did not identify any problems with the authors’ measure of success, other than lack of baselines.\n",
    "\n",
    "8.  The paper titled Overview of business innovations and research opportunities in blockchain by J.L. Zhao, S. Fan, J. Yan, published in Financial Innovation, 2. It is a review of blockchain technology research up to that point, and opportunities for continued research. It was referenced heavily in this paper, and provided not only a background survey of prior work, but areas where research was lacking. One of the main goals of this paper was to help bridge that gap. This paper wasn’t an improvement on Zhao et al, but an improvement on the body of research Zhao et al surveyed.\n",
    "\n",
    "9.  The most important algorithm used in this paper is the Latent Dirichlet Allocation (LDA). This is one of the most popular topic models in natural language processing. A topic model is a machine learning method for unsupervised classification of documents, which finds natural groups even when these groups are not already defined. It is helpful for automatically organizing, categorizing, and searching large data sets. LDA does just this; identifying potential topics based on word frequencies in documents without given topic labels. It is one of the most effective and widely used methods.\n",
    "\n",
    "10. The equation that defines topic coherence, which measures the quality, or cohesiveness and interpretability of a topic:\n",
    "\n",
    "C(t; V^(t)) = (Σ^M_m=2) (Σ^m-1_l=1) log ( (F( (v_m^(t)) , (v_1^(t)) ) + 1) /  F(v_1^(t)) )\n",
    "\n",
    "F(v) is the number of documents with at least a word v, F(v, v’) is the number of documents containing at least a word v and v’. V^(t) is a list of M words that have the highest probability of appearing in this topic. \n",
    "\n",
    "11. At first glance, our project seems completely unrelated to this paper, as ours concerns remote work in the U.S. and this paper focuses on blockchain technology in China. However, we will be dealing with the same type of data sets and looking for similar types of results. Our project’s data will also come from job postings from job sites. We will search for similar types of data; for instance, the date of the posting and certain keywords (“blockchain” in their case; “remote” in ours) in the job titles, descriptions, or requirements. We will also be using similar methods to find trends in demand and prevalence of these keywords. These include data processing, statistical methods, and regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Paper: COVID-19 and Remote Work: An Early Look at US Data\n",
    "https://www.nber.org/system/files/working_papers/w27344/w27344.pdf\n",
    "\n",
    "1. This paper is published online by the National Bureau of Economic Research (NBER) as part of their “working papers” series. This classification means that the paper has not yet been peer-reviewed or reviewed by the NBER Board of Directors, and it is not categorized as an official NBER publication. The benefit of this type of working publication is that it was published in June 2020, making information relevant to the unfolding remote work situation publicly rapidly available. Since its publication, the paper has been cited 637 times. Erik Brynjolfsson, John J. Horton, Daniel Rock,Garima Sharma, and Hong-Yi TuYe are connected through the MIT Sloan School of Management, while John J. Horton and Adam Ozimek also work for Upwork, a freelancing website. \n",
    "\n",
    "2. The authors are using nationally representative surveys to examine the lasting impact of COVID-19 on remote work as of April and May 2020. Specifically, the paper is looking at the incidence of remote work among workers in the US, how many people have shifted to remote work because of the pandemic, and what occupations and industries experienced more changes both in location and employment rate.\n",
    "\n",
    "3. This problem is important because remote vs. in-person work has significant questions of access and quality of life for people working in the US, and one of the greatest impacts of COVID-19 in the US has been its impact on employment and subsequently on the economy. Throughout the pandemic, remote vs. in-person employment has been a highly contested conversation centered around access and workers’ rights. When this study took place, the only people working in-person were “essential workers”, often people in the service industry, whose jobs were previously undervalued and who were often not compensated for the increased risk of in-person work during the pandemic. Now, for people without access to high speed internet or private workspaces, having the option to return to an office is desirable, while others prefer the independence and lack of commute working at home.\n",
    "\n",
    "4. The most mathematical notation in this paper is tables with a p-value. When reporting the results of the linear regression, the authors report “significance indicators”, p ≤ 0.1, p ≤ 0.05, and p ≤ 0.01. In this case, the p-value indicates that if the results were random, meaning if the variable was not a significant predictor of the response, there would be a probability of 0.1, 0.05, or 0.01 of getting a result as or more extreme as the actual sample values. This is a low probability, which is why the authors can conclude that the variable is a significant predictor of the dependent variable. \n",
    "\n",
    "5. Generally I felt very comfortable with the terms used in this paper. \n",
    "“Slope graph” - As the name suggests it does display slope, in this case comparing the slope for male and female responds to demonstrate that they are relatively similar. \n",
    "\n",
    "\n",
    "6. The authors collected their own data using Google Consumer Surveys in April and May of 2020. For dependent variables, the authors used the New York Times COVID cases per 100,000 individuals by state in May 2020 as well as the April 2020 unemployment rate from the Bureau of Labor Statistics. In order to provide a comparison to pre-pandemic remote work rates, the authors used the 2013-2017 American Time Use Survey, the 2019 “Freelancing in America” survey by Upwork, and the 2019 Census.\n",
    "\n",
    "This study used linear regression to examine the relationship between various dependent variables, such as the fraction of employees who have been laid off, who are commuting, and who now work from home, to predict a few different variables of interest, such as unemployment numbers, unemployment insurance claims, and changes to work. This paper did not justify their methodology or use of these models, primarily interpreting their findings in context.\n",
    "The authors initially determined there was a finding if a relationship between a predictor variable and a dependent variable were found to be significant, meaning a p-value less than 0.01. Some of the pairs of predictors and dependent variables such as Covid cases and remote work by state have an intuitive connection, so if the model had indicated that none of the predictors were significant, that would have been a sign that the model was not successful.\n",
    "Additionally, the authors knew their approach was successful because they compared their findings to preliminary research conducted by other organizations. For example, this study found that respondents of this study in older age groups were more likely to be continuing to work from home, which the researchers found consistent with the results of the 2019 Census. Multiple datasets collected pre-pandemic were used in this way to verify results.\n",
    "7. Evaluation metrics, what do they mean, baselines, problems with measures of success\n",
    "The evaluation metrics in this paper are p-values of variables, the meaning of which is described in response to question 4. The baselines in this study are \n",
    "\n",
    "8. This study is one of the first done regarding remote work and the COVID-19 Pandemic. One previous study cited conducted by the company Upwork described remote and freelance work in the US in 2019. The research question pursued in this study regarding the frequency and characteristics of remote work is parallel to the question asked in Upworthy’s report, however the findings are significantly different due to COVID-19. This study is the most important because it seems to be the most comprehensive study cited about remote work before the pandemic, meaning it serves as the baseline of comparison for this study in order to isolate the impact of COVID-19.\n",
    "\n",
    "9. One tool used in this paper is the method of survey distribution: Google Consumer Surveys (GCS). While the survey distributed by the researchers contained only one question, two citations demonstrate that the responses represent population information in a way comparable to other methods of surveying. GCS specifically captures the population according to important variables of interest, such as age group and location. This methodology is crucial to the study, because this information about remote work is primarily interesting in its generalizability to the population of the US.\n",
    "\n",
    "10. There are no equations in this paper. The paper uses linear regression, but the authors do not explain the process. \n",
    "\n",
    "11. There are two primary differences between our project and this paper. The first is that our study is taking place almost two years later, meaning that it is studying the lasting and not the immediate impacts of COVID-19 on remote work. Examining the long-term changes in work and labor will allow for insight into what might become the “new normal”. Additionally, this paper uses survey data while ours uses job listings. As a result, this study is centered around people who are employed, while ours will examine the jobs available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth Paper: Is Internet Job Search Still Ineffective?\n",
    "https://doi.org/10.1111/ecoj.12119\n",
    "\n",
    "1. This paper was published in 2014 by The Economic Journal, a well respected journal released by Oxford University Press. Both authors are professors of economics, Peter Kuhn at UC Santa Barbara, specifically studying labor economics, and Hani Mansour at University of Colorado Denver. The paper has been cited 354 times. \n",
    "\n",
    "2. This study is examining the claim made by previous papers from the mid 2000’s that found that the Internet had no impact on labor market matching or the quality of those matches. This claim is evaluated by conducting a new study based on how people in the US looked for jobs between 2005-2008, as well as how long they stayed in those positions. \n",
    "\n",
    "3. The internet has had a significant positive impact on reducing myriad “search frictions”, including apartment rentals and life insurance. One further extension of this benefit of the internet is in the job market. This application is especially important because good job placement and satisfaction with a job leads to a higher quality of life for employees, and can lead to higher productivity and longer tenure, both of which benefit employers. \n",
    "\n",
    "4. This paper does not contain mathematical notation.\n",
    "\n",
    "5. Unknown terms\n",
    "“Determinants” - A determinant can be used as an English word meaning a factor that affects an outcome, or as the math term based on a matrix. I wasn’t sure if the mathematical determinant played a role in regression, but I believe they just mean variables that affect the outcome.\n",
    "“Cox Proportional Hazards Regression” - This methodology is not explained in depth, but a figure is included referencing unemployment time for people who use internet job search and those who don’t. The Cox Regression is a survival analysis, meaning that the result occurs over time.\n",
    "“Partial-equilibrium” - This is an economic term that refers to holding most things constant and just changing one thing to isolate its effect. \n",
    "\n",
    "6. The datasets used in this study are the National Longitudinal Survey of Youth (NLSY97) collected between 2005 and 2008 and the Census’ current population survey. Both are nationally representative, meaning the results of analysis on this dataset can be generalized to all people in the United States between the ages of 23 and 29.\n",
    "\n",
    "The authors use a few different models. One analysis uses a “linear probability model”, which we know as logistic regression, justified because the value being predicted is a binary outcome, 1 if the respondent is looking for work and 0 if they are not. Continuous variables like duration of unemployment are predicted with a linear regression model. The authors know their approach is successful based on the p-value associated with the variable. Since they are looking to explain trends in job searches and job patterns, they are looking for a model with explanatory variables that are significant predictors, meaning at least one of the variables needs to have a significant p-value. Otherwise, there isn’t necessarily a clear and objective measure of success. Whether or not the trends had changed since the previous studies, the result is still interesting and successful.\n",
    "\n",
    "7. The authors also use a p-value to measure the significance of their variables. The baseline being compared to is the previous papers cited from the mid 2000’s, however the results are not expected to be the same as previously. I do not see a problem with their measure of success.\n",
    "\n",
    "8. This paper is a direct response to prior research on the impact of the internet on the job search process. The past study that seems to be cited the most is one by Kuhn and Skuterund in 2004 about Internet Job Search and Unemployment Durations that found that the internet did not reduce unemployment durations, meaning it did not help people fine employment more quickly. This paper also uses parallel methodology to the Kuhn and Skuterund survey in order to demonstrate clearly that the counterproductive effect of the internet on the job search process is more than reversed after 10 years, meaning the internet has a positive effect on the job search process.\n",
    "\n",
    "9. Tools, methods, algorithms\n",
    "This paper does not specifically cite a paper as the basis for methodology because the methodology used is linear and logistic regression, both of which are widely accepted and used methods. The authors mention that they use the same “regression specification” as the Kuhn and Skuterund study, however they do not describe what these specifications are. \n",
    "\n",
    "10. This paper does not contain an equation.\n",
    "\n",
    "11. This paper is different from our project in both focus and methodology, but it situates our research of Indeed.com and its importance in making the job search process easier. Even in 2014 the internet was an effective method of connecting people to jobs that are better fits for them. As a result, studying job trends using online job postings, like those on Indeed.com, can provide valuable insight on the current job market and the positions that are available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
